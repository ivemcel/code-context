import {
    Splitter,
    CodeChunk,
    AstCodeSplitter
} from './splitter';
import {
    Embedding,
    EmbeddingVector,
    OpenAIEmbedding
} from './embedding';
import {
    VectorDatabase,
    VectorDocument,
    VectorSearchResult
} from './vectordb';
import { SemanticSearchResult } from './types';
import * as fs from 'fs';
import * as path from 'path';
import * as crypto from 'crypto';
import { FileSynchronizer } from './sync/synchronizer';
import { generateCode } from './utils/gpt4-client';

const DEFAULT_SUPPORTED_EXTENSIONS = [
    // Programming languages
    '.java',
//    '.ts', '.tsx', '.js', '.jsx', '.py', '.java', '.cpp', '.c', '.h', '.hpp',
//    '.cs', '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala', '.m', '.mm',
    // Text and markup files
    // '.md', '.markdown', '.ipynb',
    // '.txt',  '.json', '.yaml', '.yml', '.xml', '.html', '.htm',
    // '.css', '.scss', '.less', '.sql', '.sh', '.bash', '.env'
];

const DEFAULT_IGNORE_PATTERNS = [
    // Common build output and dependency directories
    'node_modules/**',
    'dist/**',
    'build/**',
    'out/**',
    'target/**',
    'coverage/**',
    '.nyc_output/**',

    // IDE and editor files
    '.vscode/**',
    '.idea/**',
    '*.swp',
    '*.swo',

    // Version control
    '.git/**',
    '.svn/**',
    '.hg/**',

    // Cache directories
    '.cache/**',
    '__pycache__/**',
    '.pytest_cache/**',

    // Logs and temporary files
    'logs/**',
    'tmp/**',
    'temp/**',
    '*.log',

    // Environment and config files
    '.env',
    '.env.*',
    '*.local',

    // Minified and bundled files
    '*.min.js',
    '*.min.css',
    '*.min.map',
    '*.bundle.js',
    '*.bundle.css',
    '*.chunk.js',
    '*.vendor.js',
    '*.polyfills.js',
    '*.runtime.js',
    '*.map', // source map files

    '.specstory/**',
    '.star-factory/**',
    '.cursor/**',
    '/docs/**',
    '/test/**',
    // æ‰€æœ‰.å¼€å¤´çš„
    '.*/**'

];

export interface CodeIndexerConfig {
    embedding?: Embedding;
    vectorDatabase?: VectorDatabase;
    codeSplitter?: Splitter;
    supportedExtensions?: string[];
    ignorePatterns?: string[];
    enableSparseVectors?: boolean; // Add option to enable sparse vector indexing
}

export class CodeIndexer {
    private embedding: Embedding;
    private vectorDatabase: VectorDatabase;
    private codeSplitter: Splitter;
    private supportedExtensions: string[];
    private ignorePatterns: string[];
    private synchronizers = new Map<string, FileSynchronizer>();
    private enableSparseVectors: boolean; // Flag for sparse vector support

    constructor(config: CodeIndexerConfig = {}) {
        // Initialize services
        this.embedding = config.embedding || new OpenAIEmbedding({
            apiKey: process.env.OPENAI_API_KEY || 'your-openai-api-key',
            model: 'text-embedding-3-small',
            ...(process.env.OPENAI_BASE_URL && { baseURL: process.env.OPENAI_BASE_URL })
        });

        if (!config.vectorDatabase) {
            throw new Error('VectorDatabase is required. Please provide a vectorDatabase instance in the config.');
        }
        this.vectorDatabase = config.vectorDatabase;

        this.codeSplitter = config.codeSplitter || new AstCodeSplitter(2500, 300);

        this.supportedExtensions = config.supportedExtensions || DEFAULT_SUPPORTED_EXTENSIONS;
        this.ignorePatterns = config.ignorePatterns || DEFAULT_IGNORE_PATTERNS;
        
        // Initialize sparse vector support
        this.enableSparseVectors = config.enableSparseVectors || false;
        
        // If using MilvusVectorDatabase, pass the enableSparseVectors option
        if (this.enableSparseVectors && this.vectorDatabase.constructor.name.includes('Milvus')) {
            // Update the vector database configuration if it's a Milvus database
            const milvusDb = this.vectorDatabase as any;
            if (milvusDb && milvusDb.config) {
                milvusDb.config.enableSparseVectors = true;
            }
        }
    }

    /**
     * Generate collection name based on codebase path
     */
    private getCollectionName(codebasePath: string): string {
        const normalizedPath = path.resolve(codebasePath);
        const hash = crypto.createHash('md5').update(normalizedPath).digest('hex');
        return `code_chunks_${hash.substring(0, 8)}`;
    }

    /**
     * Index entire codebase
     * @param codebasePath Codebase path
     * @param progressCallback Optional progress callback function
     * @returns Indexing statistics
     */
    async indexCodebase(
        codebasePath: string,
        progressCallback?: (progress: { phase: string; current: number; total: number; percentage: number }) => void
    ): Promise<{ indexedFiles: number; totalChunks: number }> {
        console.log(`ğŸš€ Starting to index codebase: ${codebasePath}`);

        // 1. Check and prepare vector collection
        progressCallback?.({ phase: 'Preparing collection...', current: 0, total: 100, percentage: 0 });
        await this.prepareCollection(codebasePath);

        // 2. Recursively traverse codebase to get all supported files
        progressCallback?.({ phase: 'Scanning files...', current: 5, total: 100, percentage: 5 });
        const codeFiles = await this.getCodeFiles(codebasePath);
        console.log(`ğŸ“ Found ${codeFiles.length} code files`);

        if (codeFiles.length === 0) {
            progressCallback?.({ phase: 'No files to index', current: 100, total: 100, percentage: 100 });
            return { indexedFiles: 0, totalChunks: 0 };
        }

        // 3. Process each file with streaming chunk processing
        // Reserve 10% for preparation, 90% for actual indexing
        const indexingStartPercentage = 10;
        const indexingEndPercentage = 100;
        const indexingRange = indexingEndPercentage - indexingStartPercentage;

        const result = await this.processFileList(
            codeFiles,
            codebasePath,
            (filePath, fileIndex, totalFiles) => {
                // Calculate progress percentage
                const progressPercentage = indexingStartPercentage + (fileIndex / totalFiles) * indexingRange;

                console.log(`ğŸ“Š Processed ${fileIndex}/${totalFiles} files`);
                progressCallback?.({
                    phase: `Processing files (${fileIndex}/${totalFiles})...`,
                    current: fileIndex,
                    total: totalFiles,
                    percentage: Math.round(progressPercentage)
                });
            }
        );

        console.log(`âœ… Codebase indexing completed! Processed ${result.processedFiles} files in total, generated ${result.totalChunks} code chunks`);

        progressCallback?.({
            phase: 'Indexing complete!',
            current: result.processedFiles,
            total: codeFiles.length,
            percentage: 100
        });

        return {
            indexedFiles: result.processedFiles,
            totalChunks: result.totalChunks
        };
    }

    async reindexByChange(
        codebasePath: string,
        progressCallback?: (progress: { phase: string; current: number; total: number; percentage: number }) => void
    ): Promise<{ added: number, removed: number, modified: number }> {
        const collectionName = this.getCollectionName(codebasePath);
        const synchronizer = this.synchronizers.get(collectionName);

        if (!synchronizer) {
            // To be safe, let's initialize if it's not there.
            const newSynchronizer = new FileSynchronizer(codebasePath, this.ignorePatterns);
            await newSynchronizer.initialize();
            this.synchronizers.set(collectionName, newSynchronizer);
        }

        const currentSynchronizer = this.synchronizers.get(collectionName)!;

        progressCallback?.({ phase: 'Checking for file changes...', current: 0, total: 100, percentage: 0 });
        const { added, removed, modified } = await currentSynchronizer.checkForChanges();
        const totalChanges = added.length + removed.length + modified.length;

        if (totalChanges === 0) {
            progressCallback?.({ phase: 'No changes detected', current: 100, total: 100, percentage: 100 });
            console.log('âœ… No file changes detected.');
            return { added: 0, removed: 0, modified: 0 };
        }

        console.log(`ğŸ”„ Found changes: ${added.length} added, ${removed.length} removed, ${modified.length} modified.`);

        let processedChanges = 0;
        const updateProgress = (phase: string) => {
            processedChanges++;
            const percentage = Math.round((processedChanges / (removed.length + modified.length + added.length)) * 100);
            progressCallback?.({ phase, current: processedChanges, total: totalChanges, percentage });
        };

        // Handle removed files
        for (const file of removed) {
            await this.deleteFileChunks(collectionName, file);
            updateProgress(`Removed ${file}`);
        }

        // Handle modified files
        for (const file of modified) {
            await this.deleteFileChunks(collectionName, file);
            updateProgress(`Deleted old chunks for ${file}`);
        }

        // Handle added and modified files
        const filesToIndex = [...added, ...modified].map(f => path.join(codebasePath, f));

        if (filesToIndex.length > 0) {
            await this.processFileList(
                filesToIndex,
                codebasePath,
                (filePath, fileIndex, totalFiles) => {
                    updateProgress(`Indexed ${filePath} (${fileIndex}/${totalFiles})`);
                }
            );
        }

        console.log(`âœ… Re-indexing complete. Added: ${added.length}, Removed: ${removed.length}, Modified: ${modified.length}`);
        progressCallback?.({ phase: 'Re-indexing complete!', current: totalChanges, total: totalChanges, percentage: 100 });

        return { added: added.length, removed: removed.length, modified: modified.length };
    }

    private async deleteFileChunks(collectionName: string, relativePath: string): Promise<void> {
        const results = await this.vectorDatabase.query(
            collectionName,
            `relativePath == "${relativePath}"`,
            ['id']
        );

        if (results.length > 0) {
            const ids = results.map(r => r.id as string).filter(id => id);
            if (ids.length > 0) {
                await this.vectorDatabase.delete(collectionName, ids);
                console.log(`Deleted ${ids.length} chunks for file ${relativePath}`);
            }
        }
    }

    /**
     * Semantic search
     * @param codebasePath Codebase path to search in
     * @param query Search query
     * @param topK Number of results to return
     * @param threshold Similarity threshold
     */
    async semanticSearch(codebasePath: string, query: string, topK: number = 5, threshold: number = 0.5): Promise<SemanticSearchResult[]> {
        console.log(`ğŸ” Executing semantic search: "${query}" in ${codebasePath}`);

        // 1. Generate query vector
        const queryEmbedding: EmbeddingVector = await this.embedding.embed(query);

        // 2. Search in vector database
        let searchResults: VectorSearchResult[];
        
        // Check if hybrid search is available and enabled
        if (this.enableSparseVectors && typeof this.vectorDatabase.hybridSearch === 'function') {
            console.log(`ğŸ” Using hybrid search with sparse vectors for: "${query}"`);
            searchResults = await this.vectorDatabase.hybridSearch(
                this.getCollectionName(codebasePath),
                query, // Text query for sparse vector search
                queryEmbedding.vector, // Dense vector for semantic search
                { topK, threshold }
            );
        } else {
            // Fall back to standard vector search
            searchResults = await this.vectorDatabase.search(
                this.getCollectionName(codebasePath),
                queryEmbedding.vector,
                { topK, threshold }
            );
        }

        // 3. Convert to semantic search result format
        const results: SemanticSearchResult[] = searchResults.map(result => ({
            content: result.document.content,
            relativePath: result.document.relativePath,
            startLine: result.document.startLine,
            endLine: result.document.endLine,
            language: result.document.metadata.language || 'unknown',
            score: result.score
        }));

        console.log(`âœ… Found ${results.length} relevant results with ${this.enableSparseVectors ? 'hybrid' : 'semantic'} search`);
        return results;
    }

    /**
     * Check if index exists for codebase
     * @param codebasePath Codebase path to check
     * @returns Whether index exists
     */
    async hasIndex(codebasePath: string): Promise<boolean> {
        const collectionName = this.getCollectionName(codebasePath);
        return await this.vectorDatabase.hasCollection(collectionName);
    }

    /**
     * Clear index
     * @param codebasePath Codebase path to clear index for
     * @param progressCallback Optional progress callback function
     */
    async clearIndex(
        codebasePath: string,
        progressCallback?: (progress: { phase: string; current: number; total: number; percentage: number }) => void
    ): Promise<void> {
        console.log(`ğŸ§¹ Cleaning index data for ${codebasePath}...`);

        progressCallback?.({ phase: 'Checking existing index...', current: 0, total: 100, percentage: 0 });

        const collectionName = this.getCollectionName(codebasePath);
        console.log(`ğŸ” collectionName: ${collectionName}`);
        
        const collectionExists = await this.vectorDatabase.hasCollection(collectionName);
        console.log(`ğŸ” collectionExists: ${collectionExists}`);
        progressCallback?.({ phase: 'Removing index data...', current: 50, total: 100, percentage: 50 });

        if (collectionExists) {
            await this.vectorDatabase.dropCollection(collectionName);
        }

        // Delete snapshot file
        await FileSynchronizer.deleteSnapshot(codebasePath);

        progressCallback?.({ phase: 'Index cleared', current: 100, total: 100, percentage: 100 });
        console.log('âœ… Index data cleaned');
    }

    /**
     * Update ignore patterns (merges with default patterns)
     * @param ignorePatterns Array of ignore patterns to add to defaults
     */
    updateIgnorePatterns(ignorePatterns: string[]): void {
        // Merge with default patterns, avoiding duplicates
        const mergedPatterns = [...DEFAULT_IGNORE_PATTERNS, ...ignorePatterns];
        this.ignorePatterns = [...new Set(mergedPatterns)]; // Remove duplicates
        console.log(`ğŸš« Updated ignore patterns: ${ignorePatterns.length} from .gitignore + ${DEFAULT_IGNORE_PATTERNS.length} default = ${this.ignorePatterns.length} total patterns`);
    }

    /**
     * Reset ignore patterns to defaults only
     */
    resetIgnorePatternsToDefaults(): void {
        this.ignorePatterns = [...DEFAULT_IGNORE_PATTERNS];
        console.log(`ğŸ”„ Reset ignore patterns to defaults: ${this.ignorePatterns.length} patterns`);
    }

    /**
     * Update embedding instance
     * @param embedding New embedding instance
     */
    updateEmbedding(embedding: Embedding): void {
        this.embedding = embedding;
        console.log(`ğŸ”„ Updated embedding provider: ${embedding.getProvider()}`);
    }

    /**
     * Update vector database instance
     * @param vectorDatabase New vector database instance
     */
    updateVectorDatabase(vectorDatabase: VectorDatabase): void {
        this.vectorDatabase = vectorDatabase;
        console.log(`ğŸ”„ Updated vector database`);
    }

    /**
     * Update splitter instance
     * @param splitter New splitter instance
     */
    updateSplitter(splitter: Splitter): void {
        this.codeSplitter = splitter;
        console.log(`ğŸ”„ Updated splitter instance`);
    }

    /**
     * Prepare vector collection
     */
    private async prepareCollection(codebasePath: string): Promise<void> {
        // Create new collection
        const collectionName = this.getCollectionName(codebasePath);

        // For Ollama embeddings, ensure dimension is detected before creating collection
        if (this.embedding.getProvider() === 'Ollama' && typeof (this.embedding as any).initializeDimension === 'function') {
            await (this.embedding as any).initializeDimension();
        }

        const dimension = this.embedding.getDimension();
        await this.vectorDatabase.createCollection(collectionName, dimension, `Code chunk vector storage collection for codebase: ${codebasePath}`);
        console.log(`âœ… Collection ${collectionName} created successfully (dimension: ${dimension})`);
    }

    /**
     * Recursively get all code files in the codebase
     */
    private async getCodeFiles(codebasePath: string): Promise<string[]> {
        const files: string[] = [];

        const traverseDirectory = async (currentPath: string) => {
            const entries = await fs.promises.readdir(currentPath, { withFileTypes: true });

            for (const entry of entries) {
                const fullPath = path.join(currentPath, entry.name);

                // Check if path matches ignore patterns
                if (this.matchesIgnorePattern(fullPath, codebasePath)) {
                    continue;
                }

                if (entry.isDirectory()) {
                    // Skip common ignored directories
                    if (!this.shouldIgnoreDirectory(entry.name)) {
                        await traverseDirectory(fullPath);
                    }
                } else if (entry.isFile()) {
                    const ext = path.extname(entry.name);
                    if (this.supportedExtensions.includes(ext)) {
                        files.push(fullPath);
                    }
                }
            }
        };

        await traverseDirectory(codebasePath);
        return files;
    }

    /**
     * Determine whether directory should be ignored
     */
    private shouldIgnoreDirectory(dirName: string): boolean {
        const ignoredDirs = [
            'node_modules', '.git', '.svn', '.hg', 'build', 'dist', 'out',
            'target', '.vscode', '.idea', '__pycache__', '.pytest_cache',
            'coverage', '.nyc_output', 'logs', 'tmp', 'temp'
        ];
        return ignoredDirs.includes(dirName) || dirName.startsWith('.');
    }

    /**
 * Process a list of files with streaming chunk processing
 * Each chunk will be combined with its source document for embedding
 * @param filePaths Array of file paths to process
 * @param codebasePath Base path for the codebase
 * @param onFileProcessed Callback called when each file is processed
 * @returns Object with processed file count and total chunk count
 */
    private async processFileList(
        filePaths: string[],
        codebasePath: string,
        onFileProcessed?: (filePath: string, fileIndex: number, totalFiles: number) => void
    ): Promise<{ processedFiles: number; totalChunks: number }> {
        const EMBEDDING_BATCH_SIZE = Math.max(1, parseInt(process.env.EMBEDDING_BATCH_SIZE || '100', 10));
        console.log(`ğŸ”§ Using EMBEDDING_BATCH_SIZE: ${EMBEDDING_BATCH_SIZE}`);

        let chunkBuffer: Array<{ chunk: CodeChunk; codebasePath: string }> = [];
        let processedFiles = 0;
        let totalChunks = 0;

        for (let i = 0; i < filePaths.length; i++) {
            const filePath = filePaths[i];

            try {
                const content = await fs.promises.readFile(filePath, 'utf-8');
                const language = this.getLanguageFromExtension(path.extname(filePath));
                const chunks = await this.codeSplitter.split(content, language, filePath);

                // Log files with many chunks or large content
                if (chunks.length > 50) {
                    console.warn(`âš ï¸  File ${filePath} generated ${chunks.length} chunks (${Math.round(content.length / 1024)}KB)`);
                } else if (content.length > 100000) {
                    console.log(`ğŸ“„ Large file ${filePath}: ${Math.round(content.length / 1024)}KB -> ${chunks.length} chunks`);
                }

                // Add chunks to buffer
                for (const chunk of chunks) {
                    chunkBuffer.push({ chunk, codebasePath });
                    totalChunks++;

                    // Process batch when buffer reaches EMBEDDING_BATCH_SIZE
                    if (chunkBuffer.length >= EMBEDDING_BATCH_SIZE) {
                        try {
                            await this.processChunkBuffer(chunkBuffer);
                        } catch (error) {
                            console.error(`âŒ Failed to process chunk batch: ${error}`);
                        } finally {
                            chunkBuffer = []; // Always clear buffer, even on failure
                        }
                    }
                }

                processedFiles++;
                onFileProcessed?.(filePath, i + 1, filePaths.length);

            } catch (error) {
                console.warn(`âš ï¸  Skipping file ${filePath}: ${error}`);
            }
        }

        // Process any remaining chunks in the buffer
        if (chunkBuffer.length > 0) {
            console.log(`ğŸ“ Processing final batch of ${chunkBuffer.length} chunks`);
            try {
                await this.processChunkBuffer(chunkBuffer);
            } catch (error) {
                console.error(`âŒ Failed to process final chunk batch: ${error}`);
            }
        }

        return { processedFiles, totalChunks };
    }

    /**
     * Generate semantic-rich Chinese comments for code chunks using GPT-4
     * @param chunks Array of code chunks
     * @param codebasePath Base path of the codebase
     * @returns Enhanced chunks with comments in their content
     */
    private async generateChunkComments(chunks: CodeChunk[], codebasePath: string): Promise<CodeChunk[]> {
        console.log(`ğŸ¤– Generating semantic-rich Chinese comments for ${chunks.length} chunks...`);
        const startTime = Date.now();
        
        try {
            // Configuration for batch processing
            const BATCH_SIZE = 10; // Process N chunks in a single API call
            const MAX_PARALLEL_BATCHES = 20; // Max parallel API calls
            console.log(`ğŸ“Š Using batch size: ${BATCH_SIZE}, max parallel batches: ${MAX_PARALLEL_BATCHES}`);
            
            // Create batches of chunks
            const batches: CodeChunk[][] = [];
            for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
                batches.push(chunks.slice(i, Math.min(i + BATCH_SIZE, chunks.length)));
            }
            
            // Create a result array pre-filled with original chunks as fallback
            const resultChunks = [...chunks];
            
            // Process batches with limited parallelism
            for (let i = 0; i < batches.length; i += MAX_PARALLEL_BATCHES) {
                const currentBatches = batches.slice(i, i + MAX_PARALLEL_BATCHES);
                const batchStartTime = Date.now();
                const batchPromises = currentBatches.map((batch, batchIndex) => 
                    this.processCommentBatch(batch, chunks, resultChunks, codebasePath, i + batchIndex, BATCH_SIZE)
                );
                
                // Wait for the current set of batches to complete
                await Promise.all(batchPromises);
                const batchDuration = Date.now() - batchStartTime;
                console.log(`âœ… Completed processing ${Math.min((i + MAX_PARALLEL_BATCHES), batches.length)}/${batches.length} batches in ${(batchDuration/1000).toFixed(2)}s`);
            }
            
            // éªŒè¯ç»“æœæ•°ç»„é•¿åº¦æ˜¯å¦ä¸è¾“å…¥æ•°ç»„ä¸€è‡´
            if (resultChunks.length !== chunks.length) {
                console.warn(`âš ï¸ è­¦å‘Š: ç»“æœå—æ•°é‡(${resultChunks.length})ä¸è¾“å…¥å—æ•°é‡(${chunks.length})ä¸ä¸€è‡´!`);
            }
            
            // éªŒè¯æ¯ä¸ªä»£ç å—æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„å¢å¼ºæ³¨é‡Š
            let missingComments = 0;
            for (let i = 0; i < resultChunks.length; i++) {
                if (resultChunks[i].content === chunks[i].content) {
                    console.warn(`âš ï¸ è­¦å‘Š: ä»£ç å— ${i} æ²¡æœ‰ç”Ÿæˆå¢å¼ºæ³¨é‡Š!`);
                    missingComments++;
                }
            }
            if (missingComments > 0) {
                console.warn(`âš ï¸ æ€»è®¡ ${missingComments} ä¸ªä»£ç å—æ²¡æœ‰ç”Ÿæˆå¢å¼ºæ³¨é‡Š!`);
            }
            
            const totalDuration = Date.now() - startTime;
            console.log(`âœ… Successfully generated comments for all chunks in ${(totalDuration/1000).toFixed(2)}s (${(totalDuration/chunks.length).toFixed(2)}ms per chunk)`);
            return resultChunks;
        } catch (error) {
            const totalDuration = Date.now() - startTime;
            console.error(`âŒ Failed to generate comments batch after ${(totalDuration/1000).toFixed(2)}s: ${error}`);
            return chunks; // Return original chunks on error
        }
    }

    /**
     * Process a single batch of chunks for comment generation
     * @param batch The batch of chunks to process
     * @param allChunks All chunks (for reference)
     * @param resultChunks The result array to update
     * @param codebasePath The codebase path
     * @param batchNumber The batch number for logging
     * @param batchSize The size of each batch
     */
    private async processCommentBatch(
        batch: CodeChunk[], 
        allChunks: CodeChunk[], 
        resultChunks: CodeChunk[], 
        codebasePath: string,
        batchNumber: number,
        batchSize: number
    ): Promise<void> {
        const batchStartTime = Date.now();
        let promptTime = 0;
        let apiCallTime = 0;
        let parseTime = 0;
        
        try {
            // Prepare batch for comment generation
            const prepStartTime = Date.now();
            // è®¡ç®—å½“å‰æ‰¹æ¬¡åœ¨æ‰€æœ‰ä»£ç å—ä¸­çš„å®é™…èµ·å§‹ç´¢å¼•ä½ç½®
            const batchStartIndex = batchNumber * batchSize;
            
            const batchPrompts = batch.map((chunk, localIndex) => {
                // ä½¿ç”¨å…¨å±€ç´¢å¼•è€Œéå±€éƒ¨ç´¢å¼•
                const globalChunkIndex = batchStartIndex + localIndex;
                
                const language = chunk.metadata.language || 'unknown';
                const relativePath = path.relative(codebasePath, chunk.metadata.filePath || '');
                let nodeType = chunk.metadata.nodeType || 'unknown';
                
                // å¦‚æœmetadataä¸­æ²¡æœ‰nodeTypeï¼Œåˆ™é€šè¿‡å†…å®¹æ¨æ–­
                if (nodeType === 'unknown' && language === 'java') {
                    if (chunk.content.includes('class ') && !chunk.content.includes('interface ')) {
                        nodeType = 'class_declaration';
                    } else if (chunk.content.includes('interface ')) {
                        nodeType = 'interface_declaration';
                    } else if (chunk.content.match(/\b[A-Z][A-Za-z0-9_]*\s*\([^)]*\)\s*(\{|throws)/)) {
                        nodeType = 'constructor_declaration';
                    } else if (chunk.content.match(/\b(public|private|protected)?\s+(static\s+)?(final\s+)?\w+(<[^>]+>)?\s+\w+\s*\([^)]*\)/)) {
                        nodeType = 'method_declaration';
                    }
                }
                
                return {
                    // å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨å…¨å±€ç´¢å¼•ï¼Œè€Œä¸æ˜¯allChunks.indexOf(chunk)æˆ–å±€éƒ¨ç´¢å¼•
                    chunkIndex: globalChunkIndex,
                    language,
                    relativePath,
                    nodeType,
                    content: chunk.content
                };
            });
            
            // Build a single batch prompt for GPT-4
            const batchPrompt = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä»£ç æ–‡æ¡£ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹${batchPrompts.length}ä¸ªä»£ç ç‰‡æ®µç”Ÿæˆè¯¦å°½ã€è¯­ä¹‰ä¸°å¯Œçš„ä¸­æ–‡æ³¨é‡Šã€‚

å¯¹æ¯ä¸ªä»£ç ç‰‡æ®µï¼Œæ³¨é‡Šéœ€è¦åŒ…æ‹¬ï¼š
1. åŠŸèƒ½æè¿°ï¼šä»£ç çš„ä¸»è¦åŠŸèƒ½å’Œç›®çš„
2. è¾“å…¥å‚æ•°ï¼šæ¯ä¸ªå‚æ•°çš„ä½œç”¨å’Œç±»å‹
3. è¿”å›ç»“æœï¼šè¿”å›å€¼çš„å«ä¹‰å’Œæ ¼å¼
4. ä¾èµ–å…³ç³»ï¼šä¸å…¶ä»–å‡½æ•°æˆ–æ¨¡å—çš„ä¾èµ–å…³ç³»ï¼ŒåŒ…æ‹¬è°ƒç”¨å…³ç³»å’Œæ•°æ®æµ

è¯·æŒ‰ç…§ä¸‹é¢çš„æ ¼å¼å›å¤ï¼Œç¡®ä¿ä¸ºæ¯ä¸ªä»£ç ç‰‡æ®µç”Ÿæˆç‹¬ç«‹çš„æ³¨é‡Šï¼Œå¹¶ä¿ç•™æ­£ç¡®çš„chunkIndexã€‚æ ¼å¼ä¸ºJSONæ•°ç»„ï¼š

\`\`\`json
[
  {
    "chunkIndex": ${batchStartIndex}, // æ³¨æ„: è¿™æ˜¯å…¨å±€ç´¢å¼•ï¼Œè€Œéå±€éƒ¨ç´¢å¼•
    "comments": "è¿™é‡Œæ˜¯å¯¹ç¬¬ä¸€ä¸ªä»£ç ç‰‡æ®µçš„è¯¦ç»†æ³¨é‡Š..."
  },
  {
    "chunkIndex": ${batchStartIndex + 1}, // è¯·æ ¹æ®å…¨å±€ç´¢å¼•é€’å¢
    "comments": "è¿™é‡Œæ˜¯å¯¹ç¬¬äºŒä¸ªä»£ç ç‰‡æ®µçš„è¯¦ç»†æ³¨é‡Š..."
  },
  // ä»¥æ­¤ç±»æ¨...
]
\`\`\`

ä¸‹é¢æ˜¯ä»£ç ç‰‡æ®µï¼š

${batchPrompts.map((prompt, index) => `
--- ä»£ç ç‰‡æ®µ ${prompt.chunkIndex} (æ¥è‡ª ${prompt.relativePath}) ---
è¯­è¨€: ${prompt.language}
èŠ‚ç‚¹ç±»å‹: ${prompt.nodeType}
ä¾èµ–æç¤º: ${prompt.language === 'java' ? this.getJavaDependencyPrompt(prompt.nodeType) : 'ä¸å…¶ä»–å‡½æ•°æˆ–æ¨¡å—çš„ä¾èµ–å…³ç³»'}

\`\`\`
${prompt.content}
\`\`\`
`).join('\n\n')}`;

            promptTime = Date.now() - prepStartTime;
            console.log(`ğŸ”„ Processing batch #${batchNumber} with ${batch.length} chunks (prompt prep: ${promptTime}ms)...`);
            
            // Call GPT-4 API once for the batch
            const apiStartTime = Date.now();
            const batchCommentsResponse = await generateCode(batchPrompt, 'gpt-4', 32000, 0);
            //console.log(`ğŸ”„ batchCommentsResponse: ${batchCommentsResponse}`);
            apiCallTime = Date.now() - apiStartTime;
            const duration = Date.now() - batchStartTime;
            console.log(`â±ï¸ Batch #${batchNumber} API call completed in ${(apiCallTime/1000).toFixed(2)}s (${(apiCallTime/batch.length).toFixed(0)}ms/chunk)`);
            
            // Parse the JSON response
            const parseStartTime = Date.now();
            let commentsData: { chunkIndex: number; comments: string }[] = [];
            try {
                // Extract JSON array from the response
                const jsonMatch = batchCommentsResponse.match(/```json\n([\s\S]*?)\n```/) || 
                                 batchCommentsResponse.match(/```\n([\s\S]*?)\n```/) ||
                                 [null, batchCommentsResponse];
                
                const jsonContent = jsonMatch ? jsonMatch[1] : batchCommentsResponse;
                commentsData = JSON.parse(jsonContent);

                // éªŒè¯è¿”å›æ•°æ®ï¼Œç¡®ä¿ä¸æ‰¹æ¬¡å¯¹åº”
                console.log(`ğŸ” éªŒè¯æ‰¹æ¬¡#${batchNumber} APIè¿”å›: æ”¶åˆ°${commentsData.length}ä¸ªç»“æœ, åº”è¯¥å¤„ç†ç´¢å¼•èŒƒå›´[${batchStartIndex}-${batchStartIndex + batch.length - 1}]`);
                
                // æ£€æŸ¥å¹¶ä¿®å¤ç´¢å¼• - å°½ç®¡æˆ‘ä»¬è¯·æ±‚ä½¿ç”¨å…¨å±€ç´¢å¼•ï¼Œä½†ä»éœ€é˜²æ­¢APIä¸éµå¾ªæŒ‡ä»¤
                for (let i = 0; i < commentsData.length; i++) {
                    const expectedIndex = batchStartIndex + i;
                    if (commentsData[i].chunkIndex !== expectedIndex) {
                        console.warn(`âš ï¸ ç´¢å¼•ä¸åŒ¹é…: APIè¿”å›chunkIndex=${commentsData[i].chunkIndex}, é¢„æœŸ=${expectedIndex}, å·²ä¿®æ­£`);
                        commentsData[i].chunkIndex = expectedIndex;
                    }
                }
                
                // ç¡®ä¿æ‰€æœ‰æ‰¹æ¬¡ä¸­çš„ä»£ç å—éƒ½æœ‰å¯¹åº”çš„è¯„è®º
                if (commentsData.length < batch.length) {
                    console.warn(`âš ï¸ è­¦å‘Š: APIè¿”å›ç»“æœæ•°é‡(${commentsData.length})å°äºæ‰¹æ¬¡å¤§å°(${batch.length})!`);
                    // ä¸ºç¼ºå¤±çš„ä»£ç å—åˆ›å»ºå ä½è¯„è®º
                    for (let i = 0; i < batch.length; i++) {
                        const expectedIndex = batchStartIndex + i;
                        if (!commentsData.some(data => data.chunkIndex === expectedIndex)) {
                            console.warn(`âš ï¸ ä¸ºç¼ºå¤±çš„ä»£ç å—ç´¢å¼• ${expectedIndex} åˆ›å»ºå ä½è¯„è®º`);
                            commentsData.push({
                                chunkIndex: expectedIndex,
                                comments: `âš ï¸ ç”±äºAPIå“åº”ä¸å®Œæ•´ï¼Œæœªèƒ½è·å–æ­¤ä»£ç å—çš„è¯¦ç»†æ³¨é‡Šã€‚`
                            });
                        }
                    }
                }
            } catch (error) {
                console.error(`Failed to parse comments JSON for batch #${batchNumber}: ${error}`);
                // Fallback: treat the entire response as a single comment for all chunks
                commentsData = batchPrompts.map(p => ({ 
                    chunkIndex: p.chunkIndex, 
                    comments: `è§£æå¤±è´¥ï¼ŒåŸå§‹å›å¤:\n${batchCommentsResponse}` 
                }));
            }
            
            // Apply the comments to the result chunks
            for (const commentData of commentsData) {
                const chunkIndex = commentData.chunkIndex;
                if (chunkIndex >= 0 && chunkIndex < allChunks.length) {
                    const originalChunk = allChunks[chunkIndex];
                    
                    // è®°å½•å¯¹åº”å…³ç³»ï¼Œç¡®ä¿æ¯ä¸ªä»£ç å—éƒ½æœ‰æ­£ç¡®çš„å¢å¼ºæ³¨é‡Š
                    const relativePath = path.relative(codebasePath, originalChunk.metadata.filePath || '');
                    console.log(`ğŸ”„ å¤„ç†ä»£ç å— ${chunkIndex}: ${relativePath}, èŠ‚ç‚¹ç±»å‹: ${originalChunk.metadata.nodeType || 'unknown'}${originalChunk.metadata.nodeName ? `, åç§°: ${originalChunk.metadata.nodeName}` : ''}`);
                    
                    // æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªä»£ç å—ï¼Œé¿å…é‡å¤å¤„ç†
                    if (resultChunks[chunkIndex].content !== allChunks[chunkIndex].content) {
                        console.log(`âš ï¸ ä»£ç å— ${chunkIndex} å·²è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡é‡å¤å¤„ç†`);
                        continue;
                    }
                    
                    resultChunks[chunkIndex] = {
                        content: `# ä»£ç å—: ${chunkIndex}
## å…ƒæ•°æ®
- æ–‡ä»¶: ${path.basename(originalChunk.metadata.filePath || '')}
- è¯­è¨€: ${originalChunk.metadata.language || 'unknown'}
- ç±»å‹: ${originalChunk.metadata.nodeType || 'unknown'}${originalChunk.metadata.nodeName ? `\n- åç§°: ${originalChunk.metadata.nodeName}` : ''}

## åŠŸèƒ½æ‘˜è¦
${commentData.comments.split('\n')[0] || ''}

## åŸå§‹ä»£ç 
\`\`\`${originalChunk.metadata.language || ''}
${originalChunk.content}
\`\`\`

## è¯¦ç»†æ³¨é‡Š
${commentData.comments}`,
                        metadata: { ...originalChunk.metadata }
                    };
                } else {
                    console.warn(`âš ï¸ è­¦å‘Š: æ”¶åˆ°æ— æ•ˆçš„chunkIndex ${chunkIndex}, è¶…å‡ºèŒƒå›´ [0, ${allChunks.length - 1}]`);
                }
            }
            parseTime = Date.now() - parseStartTime;
            
            const totalDuration = Date.now() - batchStartTime;
            console.log(`âœ… Processed batch #${batchNumber} with ${batch.length} chunks in ${(totalDuration/1000).toFixed(2)}s
            - Prompt preparation: ${(promptTime/1000).toFixed(2)}s (${Math.round(promptTime/totalDuration*100)}%)
            - API call: ${(apiCallTime/1000).toFixed(2)}s (${Math.round(apiCallTime/totalDuration*100)}%)
            - Parse & process: ${(parseTime/1000).toFixed(2)}s (${Math.round(parseTime/totalDuration*100)}%)
            - Per chunk: ${(totalDuration/batch.length).toFixed(0)}ms`);
        } catch (error) {
            const duration = Date.now() - batchStartTime;
            console.error(`âŒ Failed to generate comments for batch #${batchNumber} after ${(duration/1000).toFixed(2)}s: ${error}`);
            // Original chunks are already in the resultChunks array as fallback
        }
    }

    /**
     * Get Java dependency prompt based on node type
     * @param nodeType Type of Java node (class, method, interface, constructor)
     * @returns Specific dependency prompt
     */
    private getJavaDependencyPrompt(nodeType: string): string {
        switch (nodeType) {
            case 'class_declaration':
                return 'è¯¦ç»†åˆ†æç±»çš„ç»§æ‰¿å…³ç³»ã€å®ç°çš„æ¥å£ã€ç±»çš„ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼šç±»æˆå‘˜ã€å†…éƒ¨ç±»ã€ä½¿ç”¨çš„å…¶ä»–ç±»ï¼‰';
            case 'method_declaration':
                return 'è¯¦ç»†åˆ†ææ–¹æ³•è°ƒç”¨çš„å…¶ä»–æ–¹æ³•æˆ–æœåŠ¡ã€ä½¿ç”¨çš„ç±»æˆ–å¯¹è±¡ã€è°ƒç”¨é“¾è·¯ã€æ“ä½œçš„æ•°æ®ç»“æ„';
            case 'interface_declaration':
                return 'è¯¦ç»†åˆ†ææ¥å£çš„ç»§æ‰¿å…³ç³»ã€å®šä¹‰çš„æ–¹æ³•å£°æ˜ã€å“ªäº›ç±»å®ç°äº†è¯¥æ¥å£ã€æ¥å£ä¸å…¶ä»–æ¥å£çš„å…³ç³»';
            case 'constructor_declaration':
                return 'è¯¦ç»†åˆ†ææ„é€ å‡½æ•°åˆå§‹åŒ–çš„å¯¹è±¡ä¾èµ–ã€æ³¨å…¥çš„æœåŠ¡ã€è°ƒç”¨çš„å…¶ä»–æ„é€ å‡½æ•°ã€åˆ›å»ºçš„å¯¹è±¡ç”Ÿå‘½å‘¨æœŸ';
            default:
                return 'ä¸å…¶ä»–æ¨¡å—æˆ–å‡½æ•°çš„ä¾èµ–å…³ç³»ã€è°ƒç”¨é“¾è·¯å’Œæ•°æ®æµ';
        }
    }

    /**
     * Process accumulated chunk buffer
     */
    private async processChunkBuffer(chunkBuffer: Array<{ chunk: CodeChunk; codebasePath: string }>): Promise<void> {
        if (chunkBuffer.length === 0) return;

        // Extract chunks and ensure they all have the same codebasePath
        const chunks = chunkBuffer.map(item => item.chunk);
        const codebasePath = chunkBuffer[0].codebasePath;

        // Estimate tokens (rough estimation: 1 token â‰ˆ 4 characters)
        const estimatedTokens = chunks.reduce((sum, chunk) => sum + Math.ceil(chunk.content.length / 4), 0);
        
        console.log(`ğŸ”„ Processing batch of ${chunks.length} chunks (~${estimatedTokens} tokens)`);
        const startTime = Date.now();
        
        try {
            // Generate semantic-rich Chinese comments for chunks - only if enabled
            const ENABLE_COMMENTS = process.env.ENABLE_COMMENTS === 'true';
            let enhancedChunks: CodeChunk[] = chunks;
            
            if (ENABLE_COMMENTS) {
                console.log(`ğŸ¤– Generating comments enabled - processing chunks with comments`);
                const commentStartTime = Date.now();
                enhancedChunks = await this.generateChunkComments(chunks, codebasePath);
                const commentDuration = Date.now() - commentStartTime;
                console.log(`ğŸ“Š Comment generation completed in ${(commentDuration/1000).toFixed(2)}s`);
            }
            
            // Process chunks with comments
            const embeddingStartTime = Date.now();
            await this.processChunkBatch(codebasePath, chunks, enhancedChunks);
            const embeddingDuration = Date.now() - embeddingStartTime;
            const totalDuration = Date.now() - startTime;
            
            console.log(`ğŸ“ˆ Performance stats: 
            - Total processing time: ${(totalDuration/1000).toFixed(2)}s
            - Embedding time: ${(embeddingDuration/1000).toFixed(2)}s (${Math.round(embeddingDuration/totalDuration*100)}%)
            - Avg. time per chunk: ${(totalDuration/chunks.length).toFixed(2)}ms`);
            
        } catch (error) {
            const duration = Date.now() - startTime;
            console.error(`âŒ Failed during chunk processing after ${(duration/1000).toFixed(2)}s: ${error}`);
            // Fallback to processing without comments
            await this.processChunkBatch(codebasePath, chunks);
        }
    }

    /**
     * Process a batch of chunks
     */
    private async processChunkBatch(codebasePath: string, chunks: CodeChunk[], enhancedChunks?: CodeChunk[]): Promise<void> {
        // Generate embedding vectors
        const chunkContents = chunks.map(chunk => chunk.content);
        //console.log(`ğŸ”„ chunkContents: ${chunkContents}`);

        // å¦‚æœenhancedChunksä¸ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨enhancedChunksçš„content
        const enhancedChunkContents = enhancedChunks ? enhancedChunks.map(chunk => chunk.content) : chunkContents;
        console.log(`ğŸ”„ å¤„ç†å¢å¼ºåçš„ä»£ç å—: ${enhancedChunks ? enhancedChunks.length : 0} ä¸ª`);
        
        // éªŒè¯å¢å¼ºå—ä¸åŸå§‹å—çš„æ•°é‡æ˜¯å¦ä¸€è‡´
        if (enhancedChunks && enhancedChunks.length !== chunks.length) {
            console.warn(`âš ï¸ è­¦å‘Š: å¢å¼ºå—æ•°é‡(${enhancedChunks.length})ä¸åŸå§‹å—æ•°é‡(${chunks.length})ä¸ä¸€è‡´!`);
        }
        
        // ä¸è¦ç›´æ¥æ‰“å°æ•´ä¸ªæ•°ç»„å†…å®¹ï¼Œé¿å…å†…å®¹æ··åˆåœ¨ä¸€èµ·
        // enhancedChunkContents.forEach(content => {
        //     console.log(`ğŸ”„ å¤„ç†å¢å¼ºåçš„ä»£ç å—: ${content}`);
        // });

        // å¯¹enhancedChunkContentsè¿›è¡Œembeddingï¼›colelctionä¸­å­˜çš„è¿˜æ˜¯åŸå§‹chunk(åŒ…æ‹¬åŸæœ‰æ³¨é‡Š)
        const embeddings: EmbeddingVector[] = await this.embedding.embedBatch(enhancedChunkContents);

        // Prepare vector documents
        const documents: VectorDocument[] = chunks.map((chunk, index) => {
            if (!chunk.metadata.filePath) {
                throw new Error(`Missing filePath in chunk metadata at index ${index}`);
            }

            const relativePath = path.relative(codebasePath, chunk.metadata.filePath);
            const fileExtension = path.extname(chunk.metadata.filePath);

            // Extract metadata that should be stored separately
            const { filePath, startLine, endLine, ...restMetadata } = chunk.metadata;

            return {
                id: this.generateId(relativePath, chunk.metadata.startLine || 0, chunk.metadata.endLine || 0, chunk.content),
                vector: embeddings[index].vector,
                content: chunk.content,
                relativePath,
                startLine: chunk.metadata.startLine || 0,
                endLine: chunk.metadata.endLine || 0,
                fileExtension,
                metadata: {
                    ...restMetadata,
                    codebasePath,
                    language: chunk.metadata.language || 'unknown',
                    chunkIndex: index
                }
            };
        });

        // Store to vector database
        await this.vectorDatabase.insert(this.getCollectionName(codebasePath), documents);
    }



    /**
     * Get programming language based on file extension
     */
    private getLanguageFromExtension(ext: string): string {
        const languageMap: Record<string, string> = {
            '.ts': 'typescript',
            '.tsx': 'typescript',
            '.js': 'javascript',
            '.jsx': 'javascript',
            '.py': 'python',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.h': 'c',
            '.hpp': 'cpp',
            '.cs': 'csharp',
            '.go': 'go',
            '.rs': 'rust',
            '.php': 'php',
            '.rb': 'ruby',
            '.swift': 'swift',
            '.kt': 'kotlin',
            '.scala': 'scala',
            '.m': 'objective-c',
            '.mm': 'objective-c',
            '.ipynb': 'jupyter'
        };
        return languageMap[ext] || 'text';
    }

    /**
     * Generate unique ID based on chunk content and location
     * @param relativePath Relative path to the file
     * @param startLine Start line number
     * @param endLine End line number
     * @param content Chunk content
     * @returns Hash-based unique ID
     */
    private generateId(relativePath: string, startLine: number, endLine: number, content: string): string {
        const combinedString = `${relativePath}:${startLine}:${endLine}:${content}`;
        const hash = crypto.createHash('sha256').update(combinedString, 'utf-8').digest('hex');
        return `chunk_${hash.substring(0, 16)}`;
    }

    /**
     * Read ignore patterns from file (e.g., .gitignore)
     * @param filePath Path to the ignore file
     * @returns Array of ignore patterns
     */
    static async getIgnorePatternsFromFile(filePath: string): Promise<string[]> {
        try {
            const content = await fs.promises.readFile(filePath, 'utf-8');
            return content
                .split('\n')
                .map(line => line.trim())
                .filter(line => line && !line.startsWith('#')); // Filter out empty lines and comments
        } catch (error) {
            console.warn(`âš ï¸  Could not read ignore file ${filePath}: ${error}`);
            return [];
        }
    }

    /**
     * Check if a path matches any ignore pattern
     * @param filePath Path to check
     * @param basePath Base path for relative pattern matching
     * @returns True if path should be ignored
     */
    private matchesIgnorePattern(filePath: string, basePath: string): boolean {
        if (this.ignorePatterns.length === 0) {
            return false;
        }

        const relativePath = path.relative(basePath, filePath);
        const normalizedPath = relativePath.replace(/\\/g, '/'); // Normalize path separators

        for (const pattern of this.ignorePatterns) {
            if (this.isPatternMatch(normalizedPath, pattern)) {
                return true;
            }
        }

        return false;
    }

    /**
     * Simple glob pattern matching
     * @param filePath File path to test
     * @param pattern Glob pattern
     * @returns True if pattern matches
     */
    private isPatternMatch(filePath: string, pattern: string): boolean {
        // Handle directory patterns (ending with /)
        if (pattern.endsWith('/')) {
            const dirPattern = pattern.slice(0, -1);
            const pathParts = filePath.split('/');
            return pathParts.some(part => this.simpleGlobMatch(part, dirPattern));
        }

        // Handle file patterns
        if (pattern.includes('/')) {
            // Pattern with path separator - match exact path
            return this.simpleGlobMatch(filePath, pattern);
        } else {
            // Pattern without path separator - match filename in any directory
            const fileName = path.basename(filePath);
            return this.simpleGlobMatch(fileName, pattern);
        }
    }

    /**
     * Simple glob matching supporting * wildcard
     * @param text Text to test
     * @param pattern Pattern with * wildcards
     * @returns True if pattern matches
     */
    private simpleGlobMatch(text: string, pattern: string): boolean {
        // Convert glob pattern to regex
        const regexPattern = pattern
            .replace(/[.+^${}()|[\]\\]/g, '\\$&') // Escape regex special chars except *
            .replace(/\*/g, '.*'); // Convert * to .*

        const regex = new RegExp(`^${regexPattern}$`);
        return regex.test(text);
    }

    /**
     * Get current splitter information
     */
    getSplitterInfo(): { type: string; hasBuiltinFallback: boolean; supportedLanguages?: string[] } {
        const splitterName = this.codeSplitter.constructor.name;

        if (splitterName === 'AstCodeSplitter') {
            const { AstCodeSplitter } = require('./splitter/ast-splitter');
            return {
                type: 'ast',
                hasBuiltinFallback: true,
                supportedLanguages: AstCodeSplitter.getSupportedLanguages()
            };
        } else {
            return {
                type: 'langchain',
                hasBuiltinFallback: false
            };
        }
    }

    /**
     * Check if current splitter supports a specific language
     * @param language Programming language
     */
    isLanguageSupported(language: string): boolean {
        const splitterName = this.codeSplitter.constructor.name;

        if (splitterName === 'AstCodeSplitter') {
            const { AstCodeSplitter } = require('./splitter/ast-splitter');
            return AstCodeSplitter.isLanguageSupported(language);
        }

        // LangChain splitter supports most languages
        return true;
    }

    /**
     * Get which strategy would be used for a specific language
     * @param language Programming language
     */
    getSplitterStrategyForLanguage(language: string): { strategy: 'ast' | 'langchain'; reason: string } {
        const splitterName = this.codeSplitter.constructor.name;

        if (splitterName === 'AstCodeSplitter') {
            const { AstCodeSplitter } = require('./splitter/ast-splitter');
            const isSupported = AstCodeSplitter.isLanguageSupported(language);

            return {
                strategy: isSupported ? 'ast' : 'langchain',
                reason: isSupported
                    ? 'Language supported by AST parser'
                    : 'Language not supported by AST, will fallback to LangChain'
            };
        } else {
            return {
                strategy: 'langchain',
                reason: 'Using LangChain splitter directly'
            };
        }
    }
}
